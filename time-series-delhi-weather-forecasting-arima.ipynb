{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2090,"sourceType":"datasetVersion","datasetId":1118}],"dockerImageVersionId":25114,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi guys, from past few days I am learning about Time series analysis and forcasting, So in this kernel going to share what I have learned so far. In layman terms, I can simply define it as 'time based analysis of any fact' and is required when target is mostly dependent on date & time variable.","metadata":{"_uuid":"bb1c92884d55d53ab89ead868b1f2d386998af03"}},{"cell_type":"markdown","source":"If you are a begineer then I would suggest below resources to go through, they helped me alot.\n\nhttps://www.youtube.com/watch?v=Aw77aMLj9uM\n\nhttps://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/","metadata":{"_uuid":"07735a0721ad7388d5be7ad48d2e8693b3a64f1a"}},{"cell_type":"markdown","source":"Below are the major steps that I am going to perform in this kernel : \n* **Import required libraries**\n* **Feature Engineering**\n* **Data Cleaning**\n* **Exploratory Data Analysis & Visualizations**\n* **Time series Prediction**","metadata":{"_uuid":"395071941e3fa82694416ce38cc18754aa7fbe1c"}},{"cell_type":"markdown","source":"## <a>Import required libraries</a>","metadata":{"_uuid":"e24fbf0b2fd33d8c4d32eaef2f01d7fad9f4a951"}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nimport seaborn as sns # for plot visualization\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# import os\n# print(os.listdir(\"../input\"))","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is column named **datetime_utc** in this dataset, we are going to read that as an index.","metadata":{"_uuid":"46e6775d811e9e120af76b5c7edb97c574e7e6d1"}},{"cell_type":"code","source":"weather_df = pd.read_csv('../input/testset.csv', parse_dates=['datetime_utc'], index_col='datetime_utc')\nweather_df.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a>Feature Engineering</a>","metadata":{"_uuid":"80082f03824293a87be852453b96736a498f77c6"}},{"cell_type":"markdown","source":"Here we are going to consider only few of the columns which seems important from some basic EDA and time series prediction's point of view. At the same time renaming with some better ones.","metadata":{"_uuid":"2d310a1e3313ff8d9777a7ad18f588a615e19ec3"}},{"cell_type":"code","source":"weather_df = weather_df.loc[:,[' _conds', ' _hum', ' _tempm']]\nweather_df = weather_df.rename(index=str, columns={' _conds': 'condition', ' _hum': 'humidity', ' _pressurem': 'pressure', ' _tempm': 'temprature'})\nprint(f'dataset shape (rows, columns) - {weather_df.shape}')\nweather_df.head()","metadata":{"_uuid":"60c5df9aba4211f1e9ee9c608ee46903c6dbf230","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets check dtype of all columns, \nweather_df.dtypes, weather_df.index.dtype","metadata":{"_uuid":"65d8f367f96a2723a88b41898f5f5b8c20680f43","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It shows 'index' as object type which needs to be converted to datetime otherwise we won't be able to perform scaling during time series analysis.","metadata":{"_uuid":"07e3ca3a91e676dd699f9fc7565239caa6023765"}},{"cell_type":"code","source":"weather_df.index = pd.to_datetime(weather_df.index)\nweather_df.index","metadata":{"_uuid":"74ffcf7e44e4fb59395ff1607a4ad33d9cd6b49f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems fine. Now lets check the total number (and percent) of missing values in each columns.","metadata":{"_uuid":"3ed1a8a05eb55eba291352c726f20f6186b017ac"}},{"cell_type":"markdown","source":"## <a>Data Cleaning</a>","metadata":{"_uuid":"c0308ac54e25cabfc7cdb6cc34fc59a7f4b9448f"}},{"cell_type":"code","source":"def list_and_visualize_missing_data(dataset):\n    # Listing total null items and its percent with respect to all nulls\n    total = dataset.isnull().sum().sort_values(ascending=False)\n    percent = ((dataset.isnull().sum())/(dataset.isnull().count())).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    missing_data = missing_data[missing_data.Total > 0]\n    \n    missing_data.plot.bar(subplots=True, figsize=(16,9))\n\nlist_and_visualize_missing_data(weather_df)","metadata":{"_uuid":"36451f00f83869fb7bea6af90c8f610426d50319","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not many values are missing, but it will still be great to fill the missing ones instead of removing entire row.","metadata":{"_uuid":"3bfea93188829659b4341f7f3df7bc2e0ebb97f8"}},{"cell_type":"code","source":"# will fill with previous valid value\nweather_df.ffill(inplace=True)\nweather_df[weather_df.isnull()].count()","metadata":{"_uuid":"e7618905331f01a3de0819a46fde1915bba7e3aa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weather_df.describe()","metadata":{"_uuid":"afa434aaad949a029f112cbe087b11ac8e45269b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is showing maximum temprature as 90 and max humidity as 243 which is non-realistic, so is an outlier. We need to treat these outliers.","metadata":{"_uuid":"6e078e38fad5adfa224e151558ea6fee21f46524"}},{"cell_type":"code","source":"weather_df = weather_df[weather_df.temprature < 50]\nweather_df = weather_df[weather_df.humidity <= 100]","metadata":{"_uuid":"01aceee96dd0dc0fe5c751d8df5169f94ff6659a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a>Exploratory Data Analysis & Visualizations</a>","metadata":{"_uuid":"4a69bf03d4cb53fec36a9c0b26cacbe16abc4e15"}},{"cell_type":"markdown","source":"Let's checkout the most common weather condition type in Delhi.","metadata":{"_uuid":"73047535377cc3b13554fbca17cee97ebd59d005"}},{"cell_type":"code","source":"weather_condition = (weather_df.condition.value_counts()/(weather_df.condition.value_counts().sum()))*100\nweather_condition.plot.bar(figsize=(16,9))\nplt.xlabel('Weather Conditions')\nplt.ylabel('Percent')","metadata":{"_uuid":"d2d3c495742fd8197df89746b34935d65906abad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oh, it is really bad that mostly Delhi has Hazy and smoky weather conditions, it shows the level of pollution city has.","metadata":{"_uuid":"520271afd016e2c1c6f27d8b3d6d207de525876d"}},{"cell_type":"markdown","source":"Let's see how plot for all year's temprature and humidity looks like.","metadata":{"_uuid":"ad99b85cf4615effca4d32e7e2a0984028c51cfd"}},{"cell_type":"code","source":"weather_df.plot(subplots=True, figsize=(20,12))","metadata":{"_uuid":"74215480529da6749a6e4a0c7376360a213f5093","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems overplotted, let's plot for only two years 2015 and 2016, it will give us the clear picture of seasonality and tread.","metadata":{"_uuid":"2d7e768d3ddd6a94df143240107635665ec31391"}},{"cell_type":"code","source":"weather_df['2015':'2016'].resample('D').fillna(method='pad').plot(subplots=True, figsize=(20,12))","metadata":{"_uuid":"e6d83fa34afcbd439aa105dacd503efddc86222d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it shows that mid two quarter are hotter than first and last, humidity varies accordingly.","metadata":{"_uuid":"ae69b0c10a2bee17019e0c7fdc903f262c02196f"}},{"cell_type":"code","source":"train_df = weather_df['2000':'2015'].resample('M').mean().fillna(method='pad')\ntrain_df.drop(columns='humidity', axis=1, inplace=True)\ntest_df = weather_df['2016':'2017'].resample('M').mean().fillna(method='pad')\ntest_df.drop(columns='humidity', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check Stationarity\nSo above plots shows that we do have seasonality but there is no trend. Let's check for below necessary conditions:\n\n* Constant mean\n* Constant variance\n* An auto co-variance that does not depend on time","metadata":{"_uuid":"ef73ea902f1f48e2940c34a954e044017e9661f3"}},{"cell_type":"code","source":"# check rolling mean and rolling standard deviation\ndef plot_rolling_mean_std(ts):\n    rolling_mean = ts.rolling(12).mean()\n    rolling_std = ts.rolling(12).std()\n    plt.figure(figsize=(22,10))\n\n    plt.plot(ts, label='Actual Mean')\n    plt.plot(rolling_mean, label='Rolling Mean')\n    plt.plot(rolling_std, label = 'Rolling Std')\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Mean Temperature\")\n    plt.title('Rolling Mean & Rolling Standard Deviation')\n    plt.legend()\n    plt.show()","metadata":{"_uuid":"6b90db0d9107464d32387ae761548b70478cd8e3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Augmented Dickey–Fuller test\ndef perform_dickey_fuller_test(ts):\n    result = adfuller(ts, autolag='AIC')\n    print('Test statistic: ' , result[0])\n    print('Critical Values:' ,result[4])","metadata":{"_uuid":"aadd34d0635379882191006195cc4dc1eddc76f7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In Dickey-Fuller test, we need only test_statics and critical_value to know if it is stationary or not","metadata":{"_uuid":"5ad493d6346c96b106fca0aa6d326da20fbd831c"}},{"cell_type":"code","source":"# check stationary: mean, variance(std)and adfuller test\nplot_rolling_mean_std(train_df.temprature)\nperform_dickey_fuller_test(train_df.temprature)","metadata":{"_uuid":"c774e93db59c5598c6321f456e7c96daa6265b7c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have constant Mean and Variance, and our Test statistic is less than Critical Values, so we already have stationary Time series. So our 'd' value will become 0 in ARIMA Model.\n\nConsider a case if it was non-stationary, in that case we would use below techniques to make it stationary\n\nMake Stationary\nFor non-stationary to stationary conversion, we can use any of the below technique :\n\n- Decomposing\n- Differencing\n\nHere, we are preferring Differencing because it is very straight forward. We would use below co-relation plots to identify the order of differencing","metadata":{}},{"cell_type":"code","source":"# Original Series\nplt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n\nfig, axes = plt.subplots(3, 2, sharex=True)\naxes[0, 0].plot(train_df.values); \naxes[0, 0].set_title('Original Series')\nplot_acf(train_df.values, ax=axes[0, 1])\n\n# 1st Differencing\naxes[1, 0].plot(train_df.temprature.diff().values); \naxes[1, 0].set_title('1st Order Differencing')\nplot_acf(train_df.diff().dropna().values,ax=axes[1, 1])\n\n# 2nd Differencing\naxes[2, 0].plot(train_df.temprature.diff().diff().values); \naxes[2, 0].set_title('2nd Order Differencing')\nplot_acf(train_df.diff().diff().dropna().values,ax=axes[2, 1])\n\nplt.xticks(rotation='vertical')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see here the first series itself is perfectly stationary, So we don't need any differencing here","metadata":{}},{"cell_type":"code","source":"# PACF plot of 1st differenced series\nplt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\n\nfig, axes = plt.subplots(1, 2, sharex=True)\naxes[0].plot(train_df.diff().values); axes[0].set_title('1st Differencing')\naxes[1].set(ylim=(0,5))\nplot_pacf(train_df.diff().dropna().values, ax=axes[1])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, sharex=True)\naxes[0].plot(train_df.diff().values); axes[0].set_title('1st Differencing')\naxes[1].set(ylim=(0,1.2))\nplot_acf(train_df.diff().dropna().values, ax=axes[1])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Timeseries Analysis (ARIMA Model)","metadata":{}},{"cell_type":"markdown","source":"For prediction we are going to use one of the most popular model for time series, **Autoregressive Integrated Moving Average (ARIMA)** which is a standard statistical model for time series forecast and analysis.\nAn ARIMA model can be understood by outlining each of its components as follows:\n* **Autoregression (AR) -** refers to a model that shows a changing variable that regresses on its own lagged, or prior, values.<br/>\nThe notation **AR(p)** indicates an autoregressive model of order p.\n\n    *Example* — If p is 3 the predictor for X(t) will be \n        X(t) = µ + X(t-1) + X(t-2) + X(t-3) + εt\n\n    Where ε is error term.\n* **Integrated (I) -** represents the differencing of raw observations to allow for the time series to become stationary, i.e., data values are replaced by the difference between the data values and the previous values.\n* **Moving average (MA) -** incorporates the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\n    The notation **MA(q)** refers to the moving average model of order q:<br/>\n ![image.png](attachment:image.png)\n\n    *Example* — If q is 3 the predictor for X(t) will be \n        X(t) = µ + εt + θ1.ε(t-1) + θ2.ε(t-2) + θ3.ε(t-3)\n    Here instead of difference from previous term, we take errer term (ε) obtained from the difference from past term\nNow we need to figure out the values of p and q which are parameters of ARIMA model. We use below two methods to figure out these values  -\n\n**Autocorrelation Function (ACF):** It just measures the correlation between two consecutive (lagged version). example at lag 4, ACF will compare series at time instance t1…t2 with series at instance t1–4…t2–4\n\n**Partial Autocorrelation Function (PACF):** is used to measure the degree of association between X(t) and X(t-p).","metadata":{}},{"cell_type":"code","source":"acf_lag = acf(train_df.diff().dropna().values, nlags=20)\npacf_lag = pacf(train_df.diff().dropna().values, nlags=20, method='ols')\n\nplt.figure(figsize=(22,10))\n\nplt.subplot(121)\nplt.plot(acf_lag)\nplt.axhline(y=0,linestyle='--',color='silver')\nplt.axhline(y=-1.96/np.sqrt(len(train_df.diff().values)),linestyle='--',color='silver')\nplt.axhline(y=1.96/np.sqrt(len(train_df.diff().values)),linestyle='--',color='silver')\nplt.title(\"Autocorrelation Function\")\n\nplt.subplot(122)\nplt.plot(pacf_lag)\nplt.axhline(y=0,linestyle='--',color='silver')\nplt.axhline(y=-1.96/np.sqrt(len(train_df.diff().values)),linestyle='--',color='silver')\nplt.axhline(y=1.96/np.sqrt(len(train_df.diff().values)),linestyle='--',color='silver')\nplt.title(\"Partial Autocorrelation Function\")\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These grey dotted line are confidence intervels which we are going to use to find out the value of p and q.\n\n__p__ - *the point where PACF crosses the upper confiednce level. In our case it seems to be 2. So we will take *p = 2.\n\n__q__ - the point where ACF crosses the upper confiednce level. In our case it seems to be 2. So we will take q = 2.\n\n__d__ - number of nonseasonal differences needed for stationarity. In this case we are going to take it as 0, since this series is already stationary.\n\nNow we are going fit time series for ARIMA Models. We will compare performance on the basis of RSS score and at last prefer the best one.","metadata":{}},{"cell_type":"code","source":"model = ARIMA(train_df.values, order=(2,0,2))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot residual errors\nresiduals = pd.DataFrame(model_fit.resid)\nfig, ax = plt.subplots(1,2)\nresiduals.plot(title=\"Residuals\", ax=ax[0])\nresiduals.plot(kind='kde', title='Density', ax=ax[1])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Actual vs Fitted\nmodel_fit.plot_predict(dynamic=False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Forecast\nfc, se, conf = model_fit.forecast(16, alpha=0.05)  # 95% conf\n\n# print(fc)\n# Make as pandas series\nfc_series = pd.Series(fc, index=test_df.index)\nlower_series = pd.Series(conf[:, 0], index=test_df.index)\nupper_series = pd.Series(conf[:, 1], index=test_df.index)\n\n# # Plot\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(train_df, label='training')\nplt.plot(test_df, label='actual')\nplt.plot(fc_series, label='forecast')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.15)\nplt.title('Forecast vs Actuals')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()\n# test_df.index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks Good. Next will explore auto ARIMA.\n\nPlease let me know if you have any feedback or questions re this.","metadata":{}}]}